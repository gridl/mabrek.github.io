---
layout: post
title:  "Service Flapping and Load Oscillations"
---

_Case 1:_ Users were experiencing slow page loads with periodical 5xx errors but the backend service looked OK with average cpu utilization about 50%. Usually it happens when some other service required to process the request is too slow but it was not the case. Sampling profiler attached to the backend showed nothing interesting in call stacks and time distribution between calls to other services was typical. The interesting thing was found on thread activity graph produced by the profiler. All the threads were busy doing something for 2 seconds and then idle for another 2 seconds like no requests were coming for 2 seconds out of 4. Load balancer logs showed that it was sending all the load to half of the cluster then declaring it down and switching to another half back and forth. Half of the cluster was unable to process all the load applied which lead to timeouts and being marked as down by load balancer.

_Case 2:_ JMeter reported that http latency is unusually high compared to requests executed manually via curl while the load test was running. If was possible though to catch a long response once in about 10 curl requests. Profiling didn't show any obvious bottleneck. It looked like the service was hitting cpu but average cpu usage was low. Reading jmeter csv report revealed interesting pattern. Test plan was configured with several hundred threads with [ConstantThroughputTimer](http://jmeter.apache.org/usermanual/component_reference.html#Constant_Throughput_Timer) between requests. All threads were sending requests almost at the same millisecond then sleeped for some time and then again burst of requests at the same time. The service was OK to handle 100 requests per second but micro-bursts of more than 1000 per second slowed it down. Somehow delays used by ConstantThroughputTimer got synchronized between threads and they started firing at the same time.

_Case 3:_ Maximum and 99th percentile of service response time was too high under flat request rate generated by JMeter (now it was using [UniformRandomTimer](http://jmeter.apache.org/usermanual/component_reference.html#Uniform_Random_Timer)). Initially latency data was aggregated with 10s resolution and looked like there is some oscillation in it:

![response time graph with 10s resolution]({{ site.url }}/img/flapping/elapsed10s.png)
Reaggregating data with 1s resolution confirmed the suspicion:

![response time graph with 1s resolution]({{ site.url }}/img/flapping/elapsed1s.png)
That oscillation was traced down to heavy query running every 30 seconds and slowing down the database.


What's common in all these cases is that periodic fluctuation in load caused performance degradation but it was only visible in hi-resolution data (second or milliseconds). 